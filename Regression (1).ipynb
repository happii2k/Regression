{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUMCHKSX9OUe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression Assignment**\n",
        "## 1. What is Simple Linear Regression?\n",
        "A statistical method that models the relationship between a dependent variable and a single independent variable using a straight line.\n",
        "\n",
        "##2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "##3. What does the coefficient m represent in the equation Y = mx + c?\n",
        "It represents the slope, indicating the change in Y for a one-unit change in X.\n",
        "\n",
        "##4. What does the intercept c represent in the equation Y = mx + c?\n",
        "The expected value of Y when X = 0.\n",
        "\n",
        "##5. How do we calculate the slope m in Simple Linear Regression?\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑥\n",
        "𝑦\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑥\n",
        ")\n",
        "(\n",
        "∑\n",
        "𝑦\n",
        ")\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑥\n",
        "2\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑥\n",
        ")\n",
        "2\n",
        "m=\n",
        "n(∑x\n",
        "2\n",
        " )−(∑x)\n",
        "2\n",
        "\n",
        "n(∑xy)−(∑x)(∑y)\n",
        "​\n",
        "\n",
        "\n",
        "##6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "To minimize the sum of the squared differences between observed and predicted values.\n",
        "\n",
        "##7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "R² measures how much of the variance in the dependent variable is explained by the model.\n",
        "\n",
        "##8. What is Multiple Linear Regression?\n",
        "A regression model that uses more than one independent variable to predict the dependent variable.\n",
        "\n",
        "##9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple has one independent variable, while multiple has two or more.\n",
        "\n",
        "##10. What are the key assumptions of Multiple Linear Regression?\n",
        "Similar to simple regression with additional checks for multicollinearity among predictors.\n",
        "\n",
        "##11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "When the variance of residuals is not constant. It violates assumptions and can lead to inefficient estimates.\n",
        "\n",
        "##12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Remove highly correlated predictors\n",
        "\n",
        "Use Principal Component Analysis (PCA)\n",
        "\n",
        "Use Ridge or Lasso Regression\n",
        "\n",
        "##13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Dummy Variables\n",
        "\n",
        "##14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "They capture the combined effect of two or more variables on the target variable.\n",
        "\n",
        "##15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "In multiple regression, the intercept is the value of Y when all independent variables are 0, which may not be meaningful.\n",
        "\n",
        "##16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "It indicates the rate of change in the dependent variable with respect to an independent variable.\n",
        "\n",
        "##17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "It serves as a baseline value of the dependent variable when all predictors are zero.\n",
        "\n",
        "##18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Does not indicate overfitting\n",
        "\n",
        "Cannot determine whether the coefficient estimates are biased\n",
        "\n",
        "Cannot confirm causality\n",
        "\n",
        "##19. How would you interpret a large standard error for a regression coefficient?\n",
        "It indicates that the coefficient estimate is not precise.\n",
        "\n",
        "##20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Residuals fan out or create a pattern. It can lead to inefficient estimates and invalid statistical tests.\n",
        "\n",
        "##21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "Some predictors may not be contributing meaningfully, suggesting overfitting.\n",
        "\n",
        "##22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "To improve model performance and interpretability, especially when using regularization techniques.\n",
        "\n",
        "##23. What is polynomial regression?\n",
        "A form of regression where the relationship between X and Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "##24. How does polynomial regression differ from linear regression?\n",
        "It fits a curved line rather than a straight line to the data.\n",
        "\n",
        "##25. When is polynomial regression used?\n",
        "When data shows a nonlinear relationship between the independent and dependent variables.\n",
        "\n",
        "##26. What is the general equation for polynomial regression?\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "\n",
        "##27. Can polynomial regression be applied to multiple variables?\n",
        "Yes, it becomes a multivariate polynomial regression.\n",
        "\n",
        "##28. What are the limitations of polynomial regression?\n",
        "\n",
        "Overfitting with high degrees\n",
        "\n",
        "Requires careful selection of degree\n",
        "\n",
        "Not interpretable for high-degree polynomials\n",
        "\n",
        "##29. What metric is used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "R²\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "Cross-Validation Error\n",
        "\n",
        "##30. Why is visualization important in polynomial regression?\n",
        "To understand the model’s fit and detect overfitting or underfitting visually.\n",
        "\n",
        "##31. How is polynomial regression implemented in Python?\n",
        "```\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "# Transform to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title(\"Polynomial Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "dIzxV3QI9Qg3"
      }
    }
  ]
}